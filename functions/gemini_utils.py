import os
from dotenv import load_dotenv
import google.generativeai as genai
import asyncio

# .envèª­ã¿è¾¼ã¿
load_dotenv()

# ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
try:
    GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
    if not GEMINI_API_KEY:
        raise ValueError("ç’°å¢ƒå¤‰æ•° 'GEMINI_API_KEY' ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    genai.configure(api_key=GEMINI_API_KEY)
    gemini_model = genai.GenerativeModel('gemini-2.5-flash')
except Exception as e:
    print(f"Geminiãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
    gemini_model = None

# --- é–¢æ•°å®šç¾© ---

async def validate_post_safety(text: str) -> tuple[bool, str]:
    if not gemini_model:
        return False, "AIãƒ¢ãƒ‡ãƒ«ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"

    prompt = f"""
ã‚ãªãŸã¯å°å­¦ç”Ÿå‘ã‘SNSã®å®‰å…¨æ€§ãƒã‚§ãƒƒã‚¯AIã§ã™ã€‚
ä»¥ä¸‹ã®æŠ•ç¨¿ã«ã€ã„ã˜ã‚ã€æš´åŠ›ã€å€‹äººæƒ…å ±ã€ãã®ä»–å­ä¾›ã«ä¸é©åˆ‡ãªå†…å®¹ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹åˆ¤å®šã—ã¦ãã ã•ã„ã€‚
å•é¡ŒãŒãªã‘ã‚Œã°ã€ŒOKã€ã€å•é¡ŒãŒã‚ã‚Œã°ã€ŒNG ç†ç”±ã€ã®å½¢å¼ã§ç­”ãˆã¦ãã ã•ã„ã€‚ç†ç”±ã¯ã²ã‚‰ãŒãªã€ã‚«ã‚¿ã‚«ãƒŠã®ã¿ã§ã€ç°¡å˜ãªè¨€è‘‰ã§èª¬æ˜ã—ã¦ãã ã•ã„ã€‚

æŠ•ç¨¿: "{text}"
"""
    try:
        response = await gemini_model.generate_content_async(prompt)
        result_text = response.text.strip()
        if result_text.startswith("OK"):
            return True, ""
        else:
            reason = result_text.replace("NG", "").strip()
            return False, reason if reason else "ä¸é©åˆ‡ãªå†…å®¹ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚"
    except Exception as e:
        return False, f"ã‚¨ãƒ©ãƒ¼: {e}"


async def judge_post_positivity(text: str) -> bool:
    if not gemini_model:
        return False
    prompt = f"""
ä»¥ä¸‹ã®æŠ•ç¨¿ã¯èª­ã‚“ã äººã‚’æ˜ã‚‹ã„æ°—æŒã¡ã«ã—ã¾ã™ã‹ï¼Ÿ
Yes ã‹ No ã ã‘ã§ç­”ãˆã¦ãã ã•ã„ã€‚
æŠ•ç¨¿: "{text}"
"""
    try:
        response = await gemini_model.generate_content_async(prompt)
        return "yes" in response.text.lower()
    except Exception:
        return False


async def predict_post_reactions(text: str) -> tuple[int, list[str]]:
    """
    æŠ•ç¨¿ã«å¯¾ã™ã‚‹åå¿œã‚¿ã‚¤ãƒ—ã‚’å®‰å®šç”Ÿæˆã€‚
    è»½é‡ãƒ¢ãƒ‡ãƒ«å‘ã‘ã«ã€ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šå½¢å¼ã§ã‚¿ã‚¤ãƒ—ã‚’è¿”ã™ã€‚
    """
    if not gemini_model:
        return 3, ["positive", "neutral", "neutral"]

    prompt = f"""
ã‚ãªãŸã¯SNSä¸Šã®ã‚³ãƒ¡ãƒ³ãƒˆäºˆæ¸¬AIã§ã™ã€‚
æŠ•ç¨¿ã«å¯¾ã—ã¦3ã€œ10ä»¶ã®ã‚³ãƒ¡ãƒ³ãƒˆãŒä»˜ãã¨æƒ³å®šã—ã¦ãã ã•ã„ã€‚
ã‚³ãƒ¡ãƒ³ãƒˆã‚¿ã‚¤ãƒ—ã¯ positive / negative / neutral ã®ã„ãšã‚Œã‹ã§ã™ã€‚
æŠ•ç¨¿: "{text}"
å‡ºåŠ›ä¾‹: positive, neutral, positive
"""
    try:
        response = await gemini_model.generate_content_async(prompt)
        text_result = response.text.strip()
        types = [t.strip() for t in text_result.split(",") if t.strip() in ["positive","neutral","negative"]]
        reply_count = len(types) if types else 3
        if not types:
            types = ["positive","neutral","neutral"]
        return reply_count, types
    except Exception:
        return 3, ["positive","neutral","neutral"]


async def generate_reaction_comments_bulk(text: str, reactions: list[str]) -> list[str]:
    """
    è»½é‡ãƒ¢ãƒ‡ãƒ«å‘ã‘ã«ã€1ä»¶ãšã¤ã‚³ãƒ¡ãƒ³ãƒˆç”Ÿæˆã—ã¦ãƒªã‚¹ãƒˆã«ã¾ã¨ã‚ã‚‹ã€‚
    æˆ»ã‚Šå€¤: ["ã‚³ãƒ¡ãƒ³ãƒˆæ–‡", ...]
    """
    if not gemini_model:
        return ["ã„ã„ã­ï¼ğŸ˜Š" for _ in reactions]

    comments: list[str] = []
    for r_type in reactions:
        # ã‚³ãƒ¡ãƒ³ãƒˆç”Ÿæˆ
        comment_prompt = f"""
ã‚ãªãŸã¯å°å­¦ç”Ÿã®SNSãƒ¦ãƒ¼ã‚¶ãƒ¼ã§ã™ã€‚
ä»¥ä¸‹ã®æŠ•ç¨¿ã«å¯¾ã—ã¦ã€1ä»¶ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
ã‚¿ã‚¤ãƒ—: {r_type}
ãƒ«ãƒ¼ãƒ«:
- ã²ã‚‰ãŒãªãƒ»ã‚«ã‚¿ã‚«ãƒŠãƒ»ç°¡å˜ãªæ¼¢å­—ã®ã¿
- 30æ–‡å­—ä»¥å†…
- çµµæ–‡å­—ã‚’1ã¤ä½¿ã†
- å°å­¦ç”Ÿã«ã‚‚èª­ã‚ã‚‹ã‚„ã•ã—ã„è¨€è‘‰

æŠ•ç¨¿: "{text}"
"""
        try:
            response = await gemini_model.generate_content_async(comment_prompt)
            comment_text = response.text.strip()
        except Exception:
            comment_text = "ã„ã„ã­ï¼ğŸ˜„"

        comments.append(comment_text)
    
    return comments


async def predict_post_likes(text: str) -> int:
    """
    ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æŠ•ç¨¿ã«å¯¾ã—ã¦ã€ã¤ãã¨äºˆæ¸¬ã•ã‚Œã‚‹ã€Œã„ã„ã­ã€ã®ä»¶æ•°ã‚’è¿”ã™ã€‚
    0ã€œ100ã®ç¯„å›²ã«ã‚¯ãƒªãƒƒãƒ—ã—ã¦æ•´æ•°ã§è¿”ã™ã€‚
    å¤±æ•—æ™‚ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ 3 ã‚’è¿”ã™ã€‚
    """
    if not gemini_model:
        return 3

    prompt = f"""
ã‚ãªãŸã¯SNSã®åå¿œäºˆæ¸¬AIã§ã™ã€‚
ä»¥ä¸‹ã®æŠ•ç¨¿ã«ã€ã©ã‚Œãã‚‰ã„ã®ã€Œã„ã„ã­ã€ãŒã¤ãã¨äºˆæ¸¬ã—ã¾ã™ã‹ï¼Ÿ
0ã€œ100ã®æ•°å­—ã ã‘ã§ç­”ãˆã¦ãã ã•ã„ã€‚
æŠ•ç¨¿: "{text}"
"""
    try:
        response = await gemini_model.generate_content_async(prompt)
        predicted = int(''.join(filter(str.isdigit, response.text.strip())))
        return max(0, min(100, predicted))
    except Exception:
        return 3
